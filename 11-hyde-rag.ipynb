{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE - Hypothetical Document Embeddings\n",
    "    -> Instead of performing the RAG on user query, here we put INTERMEDAITE LLM to generate an answer (hypothetical)\n",
    "    -> End user can't see this answer. this answer will compare (similarity using embeddings) with source documents.\n",
    "    -> earlier we compared the user query with the documents in the rag (query-to-answer RAG)\n",
    "       now we compare hypothetical answer with the documents in the rag (answer-to-answer RAG)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "import yaml, os, openai, textwrap, langchain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder, RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = credentials['AD_OPENAI_API_TYPE']\n",
    "os.environ[\"OPENAI_API_VERSION\"] = credentials['AD_OPENAI_API_VERSION']\n",
    "os.environ[\"OPENAI_API_BASE\"] = credentials['AD_OPENAI_API_BASE']\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials['AD_OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 01 - Single Hypothetical Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "                                        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                        model_kwargs={'device': 'mps'},\n",
    "                                        encode_kwargs={'normalize_embeddings': True}\n",
    "                                        )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "                    deployment_name=credentials['AD_DEPLOYMENT_ID'],\n",
    "                    model_name=credentials['AD_ENGINE'],\n",
    "                    temperature=0.9, \n",
    "                    max_tokens = 256\n",
    "                    )\n",
    "\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "                                                    llm,\n",
    "                                                    bge_embeddings,\n",
    "                                                    prompt_key=\"web_search\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a passage to answer the question \n",
      "Question: {QUESTION}\n",
      "Passage:\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Please write a passage to answer the question \\nQuestion: What items does McDonalds make?\\nPassage:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] [4.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"McDonald's is a global fast-food chain that prepares and serves a wide variety of food and beverages for its customers. Some of the most famous items that McDonald's is known for include their Big Mac, Quarter Pounder with Cheese, Chicken McNuggets, and Filet-O-Fish. Additionally, McDonald's offers a range of breakfast items such as Egg McMuffin, Sausage Biscuit, and Hotcakes. They also serve various beverages such as freshly brewed coffee, soft drinks, and milkshakes. Apart from these, McDonald's offers seasonal specials and limited-time menu items, such as McRib sandwich, Shamrock Shake, and various McCafe beverages. In summary, McDonald's makes a variety of popular fast-food items, and they regularly introduce new menu items to meet the changing tastes of their customers.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"McDonald's is a global fast-food chain that prepares and serves a wide variety of food and beverages for its customers. Some of the most famous items that McDonald's is known for include their Big Mac, Quarter Pounder with Cheese, Chicken McNuggets, and Filet-O-Fish. Additionally, McDonald's offers a range of breakfast items such as Egg McMuffin, Sausage Biscuit, and Hotcakes. They also serve various beverages such as freshly brewed coffee, soft drinks, and milkshakes. Apart from these, McDonald's offers seasonal specials and limited-time menu items, such as McRib sandwich, Shamrock Shake, and various McCafe beverages. In summary, McDonald's makes a variety of popular fast-food items, and they regularly introduce new menu items to meet the changing tastes of their customers.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 29,\n",
      "      \"completion_tokens\": 167,\n",
      "      \"total_tokens\": 196\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = embeddings.embed_query(\"What items does McDonalds make?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 02 - Multiple Hypothetical Generation\n",
    "\n",
    "generate multiple hypothetical answers and aggreate all of their embeddings to get a single embedding for the overall hypothetical answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "                                        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                        model_kwargs={'device': 'mps'},\n",
    "                                        encode_kwargs={'normalize_embeddings': True}\n",
    "                                        )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "                    deployment_name=credentials['AD_DEPLOYMENT_ID'],\n",
    "                    model_name=credentials['AD_ENGINE'],\n",
    "                    temperature=0.9, \n",
    "                    max_tokens = 256,\n",
    "                    n=4,      ########################################### Check this\n",
    "                    )\n",
    "\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "                                                    llm,\n",
    "                                                    bge_embeddings,\n",
    "                                                    prompt_key=\"web_search\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Please write a passage to answer the question \\nQuestion: What is McDonalds best selling item?\\nPassage:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] [2.74s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"McDonald's most popular and best-selling item, without a doubt, is the Big Mac. The iconic sandwich, which features two hamburger patties, cheese, lettuce, onions, pickles, and special sauce, has been a staple on the McDonald's menu since its debut in 1967. In fact, the Big Mac is so beloved that it has inspired a signature jingle (\\\"two all-beef patties, special sauce, lettuce, cheese, pickles, onions on a sesame seed bun\\\") and has even been the subject of a documentary. The Big Mac has remained a top-seller for decades, and its popularity shows no signs of waning anytime soon.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"McDonald's most popular and best-selling item, without a doubt, is the Big Mac. The iconic sandwich, which features two hamburger patties, cheese, lettuce, onions, pickles, and special sauce, has been a staple on the McDonald's menu since its debut in 1967. In fact, the Big Mac is so beloved that it has inspired a signature jingle (\\\"two all-beef patties, special sauce, lettuce, cheese, pickles, onions on a sesame seed bun\\\") and has even been the subject of a documentary. The Big Mac has remained a top-seller for decades, and its popularity shows no signs of waning anytime soon.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"text\": \"McDonald's best selling item is the Big Mac. This iconic burger was first introduced in 1967 and has since become a staple on the McDonald's menu. The Big Mac is made with two all-beef patties, special sauce, lettuce, cheese, pickles, onions, and a sesame seed bun. It has become a popular choice for customers all over the world and has helped solidify McDonald's as a leading fast food chain. In addition to the Big Mac, other popular items on the McDonald's menu include the Quarter Pounder with Cheese, Chicken McNuggets, and the Filet-O-Fish.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"McDonald's best selling item is the Big Mac. This iconic burger was first introduced in 1967 and has since become a staple on the McDonald's menu. The Big Mac is made with two all-beef patties, special sauce, lettuce, cheese, pickles, onions, and a sesame seed bun. It has become a popular choice for customers all over the world and has helped solidify McDonald's as a leading fast food chain. In addition to the Big Mac, other popular items on the McDonald's menu include the Quarter Pounder with Cheese, Chicken McNuggets, and the Filet-O-Fish.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"text\": \"McDonald’s best selling item is the Big Mac sandwich. The Big Mac has been a customer favorite since its introduction in 1967 and consists of two beef patties, special sauce, lettuce, cheese, pickles, and onions on a sesame seed bun. In addition to the Big Mac, McDonald’s also sells a variety of other popular items, including the Egg McMuffin breakfast sandwich, Chicken McNuggets, and the Quarter Pounder with Cheese. Despite new menu offerings and changing tastes, the Big Mac remains a beloved classic and the top selling item at McDonald’s.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"McDonald’s best selling item is the Big Mac sandwich. The Big Mac has been a customer favorite since its introduction in 1967 and consists of two beef patties, special sauce, lettuce, cheese, pickles, and onions on a sesame seed bun. In addition to the Big Mac, McDonald’s also sells a variety of other popular items, including the Egg McMuffin breakfast sandwich, Chicken McNuggets, and the Quarter Pounder with Cheese. Despite new menu offerings and changing tastes, the Big Mac remains a beloved classic and the top selling item at McDonald’s.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"text\": \"McDonald's most popular and best-selling item is undoubtedly the Big Mac. This famous burger has been a staple on the McDonald's menu since it was introduced in 1968 and has remained a fan favorite ever since. The Big Mac consists of two beef patties, special sauce, lettuce, cheese, pickles, and onions, all sandwiched between three sesame seed buns. It is an iconic fast-food item that has been the centerpiece of countless McDonald's advertising campaigns and has consistently been a top seller for the company. In fact, McDonald's estimates that they sell over 17 Big Macs every second, making it one of the most recognized burgers in the world. So whether you're a die-hard fan or a first-time customer, the Big Mac is sure to satisfy your hunger cravings and leave you wanting more.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"McDonald's most popular and best-selling item is undoubtedly the Big Mac. This famous burger has been a staple on the McDonald's menu since it was introduced in 1968 and has remained a fan favorite ever since. The Big Mac consists of two beef patties, special sauce, lettuce, cheese, pickles, and onions, all sandwiched between three sesame seed buns. It is an iconic fast-food item that has been the centerpiece of countless McDonald's advertising campaigns and has consistently been a top seller for the company. In fact, McDonald's estimates that they sell over 17 Big Macs every second, making it one of the most recognized burgers in the world. So whether you're a die-hard fan or a first-time customer, the Big Mac is sure to satisfy your hunger cravings and leave you wanting more.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 30,\n",
      "      \"completion_tokens\": 545,\n",
      "      \"total_tokens\": 575\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = embeddings.embed_query(\"What is McDonalds best selling item?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 03 - Custom Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question as a single food item\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HypotheticalDocumentEmbedder(\n",
    "                                            llm_chain=llm_chain,\n",
    "                                            base_embeddings=bge_embeddings\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Please answer the user's question as a single food item\\nQuestion: What is is McDonalds best selling item?\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] [883ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Big Mac\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Big Mac\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"text\": \"Big Mac\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Big Mac\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"text\": \"Big Mac\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Big Mac\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"text\": \"Big Mac\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Big Mac\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 33,\n",
      "      \"completion_tokens\": 8,\n",
      "      \"total_tokens\": 41\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = embeddings.embed_query(\n",
    "    \"What is is McDonalds best selling item?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use HyDE for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader('data/langchain_blog_posts/blog.langchain.dev_announcing-langsmith_.txt'),\n",
    "    TextLoader('data/langchain_blog_posts/blog.langchain.dev_benchmarking-question-answering-over-csv-data_.txt'),\n",
    "    TextLoader('data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'),\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs) #split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question as related to Large Language Models\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables=[\"question\"], \n",
    "                        template=prompt_template\n",
    "                        )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "                    deployment_name=credentials['AD_DEPLOYMENT_ID'],\n",
    "                    model_name=credentials['AD_ENGINE'],\n",
    "                    temperature=0.9, \n",
    "                    max_tokens = 256\n",
    "                    )\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "                    llm=llm,  ####################################### for Hypothetical Answer Generation\n",
    "                    prompt=prompt\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HypotheticalDocumentEmbedder(\n",
    "                                        llm_chain=llm_chain,\n",
    "                                        base_embeddings=bge_embeddings\n",
    "                                        )\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Please answer the user's question as related to Large Language Models\\nQuestion: What are chat loaders?\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] [2.22s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Chat loaders are modules or components of large language models that are specifically designed to generate conversational responses in natural language. These loaders analyze and interpret the input message/ query and use complex algorithms, machine learning, and natural language processing techniques to generate appropriate responses. These chat loaders enable language models to serve as chatbots, virtual assistants, and other conversational interfaces for various use cases such as customer support, personal assistant, education, and entertainment, among others.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Chat loaders are modules or components of large language models that are specifically designed to generate conversational responses in natural language. These loaders analyze and interpret the input message/ query and use complex algorithms, machine learning, and natural language processing techniques to generate appropriate responses. These chat loaders enable language models to serve as chatbots, virtual assistants, and other conversational interfaces for various use cases such as customer support, personal assistant, education, and entertainment, among others.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 30,\n",
      "      \"completion_tokens\": 91,\n",
      "      \"total_tokens\": 121\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are chat loaders?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\\nTitle: Chat Loaders: Fine-tune a ChatModel in your Voice\\n\\nSummary\\n\\nWe are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\\n\\nThank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\\n\\nImportant Links:\\n\\nContext\\n\\nOn Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\\n\\nWhile fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.', metadata={'source': 'data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
       " Document(page_content='URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\\nTitle: Chat Loaders: Fine-tune a ChatModel in your Voice\\n\\nSummary\\n\\nWe are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\\n\\nThank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\\n\\nImportant Links:\\n\\nContext\\n\\nOn Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\\n\\nWhile fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.', metadata={'source': 'data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
       " Document(page_content='URL: https://blog.langchain.dev/chat-loaders-finetune-a-chatmodel-in-your-voice/\\nTitle: Chat Loaders: Fine-tune a ChatModel in your Voice\\n\\nSummary\\n\\nWe are adding a new integration type, ChatLoaders, to make it easier to fine-tune models on your own unique writing style. These utilities help convert data from popular messaging platforms to chat messages compatible with fine-tuning formats like that supported by OpenAI.\\n\\nThank you to Greg Kamradt for Misbah Syed for their thought leadership on this.\\n\\nImportant Links:\\n\\nContext\\n\\nOn Tuesday, OpenAI announced improved fine-tuning support, extending the service to larger chat models like GPT-3.5-turbo. This enables anyone to customize these larger, more capable models for their own use cases. They also teased support for fine-tuning GPT-4 later this year.\\n\\nWhile fine-tuning is typically not advised for teaching an LLM substantially new knowledge or for factual recall; it is good for style transfer.', metadata={'source': 'data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'}),\n",
       " Document(page_content=\"These utilities take data exported from popular messaging platforms and convert them to LangChain message objects, which you can then easily convert platform-agnostic message formats, such as OpenAI, Llama 2, and others. This training data can be used directly for fine-tuning a model.\\n\\nWe've added loaders for the following popular messaging platforms so far:\\n\\nFacebook Messenger\\n\\nSlack\\n\\nTelegram\\n\\nWhatsApp\\n\\nWe have also added a recipe on how to do so for Discord and Twitter (using Apify) and plan to integrate additional chat loaders in the near future. If you have a favorite messaging platform you'd like to support, we'd love to help you land a PR!\\n\\nTo get you started, we've added an end-to-end example notebook to the LangChain documentation showing how to fine-tune gpt-3.5-turbo (the model behind ChatGPT) on an example set of Facebook messages.\\n\\n❗ Please ensure all participants of your conversations support the decision to train a model on the chat data before proceeding.\", metadata={'source': 'data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "                                        llm=llm, ####################################### for RAG\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever=docsearch.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                        return_source_documents=True\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What is Flash attention?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Please answer the user's question as related to Large Language Models\\nQuestion: What is Flash attention?\\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:AzureChatOpenAI] [2.78s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Flash attention is a type of attention mechanism used in large language models such as GPT-3. It is designed to improve the speed and efficiency of the attention network by allowing the model to focus on only a small portion of the input sequence at a time, rather than attending to the entire sequence in each step. This approach has been shown to significantly reduce memory requirements and improve performance on language modeling tasks.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Flash attention is a type of attention mechanism used in large language models such as GPT-3. It is designed to improve the speed and efficiency of the attention network by allowing the model to focus on only a small portion of the input sequence at a time, rather than attending to the entire sequence in each step. This approach has been shown to significantly reduce memory requirements and improve performance on language modeling tasks.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 30,\n",
      "      \"completion_tokens\": 81,\n",
      "      \"total_tokens\": 111\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is Flash attention?\",\n",
      "  \"context\": \"Why is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\\n\\nWhy is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\\n\\nWhy is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nWhy is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\\n\\nWhy is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\\n\\nWhy is this better than direct instructions? Style and tone can be hard to describe! Most of us don't write like ChatGPT, and it can sometimes be frustratingly difficult to get the LLM to consistently respond in a particular voice (especially over longer conversations).\\n\\nWhy is this better than few-shot examples? It can be challenging to capture your voice in only a few concise snippets! Fine-tuning lets you provide a larger number of examples the model can learn from without having to see them every time you want to query the model.\\n\\nChatLoaders\\n\\nAt LangChain, we want to make it as easy as possible for you to take advantage of this improved fine-tuning support. To make it simple to adapt a model to your voice, we're adding a new integration type: ChatLoaders .\\nHuman: What is Flash attention?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:AzureChatOpenAI] [595ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I'm sorry, but there is no information in the given context about Flash attention.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I'm sorry, but there is no information in the given context about Flash attention.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 545,\n",
      "      \"completion_tokens\": 17,\n",
      "      \"total_tokens\": 562\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [596ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I'm sorry, but there is no information in the given context about Flash attention.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [601ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"I'm sorry, but there is no information in the given context about Flash attention.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "I'm sorry, but there is no information in the given context about Flash attention.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt\n",
      "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt\n",
      "data/langchain_blog_posts/blog.langchain.dev_chat-loaders-finetune-a-chatmodel-in-your-voice_.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
