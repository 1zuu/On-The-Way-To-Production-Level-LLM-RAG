{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
    "!unzip -q new_papers.zip -d data\n",
    "!rm new_papers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1zuu/anaconda3/envs/llm/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import yaml, os, openai, textwrap\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = credentials['OPENAI_API_KEY']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = credentials['HUGGINGFACEHUB_API_TOKEN']\n",
    "os.environ['ENGINE'] = credentials['ENGINE']\n",
    "\n",
    "openai.api_key = credentials['OPENAI_API_KEY']\n",
    "openai.api_base = credentials['OPENAI_API_BASE']\n",
    "openai.api_type = credentials['OPENAI_API_TYPE']\n",
    "openai.api_version = credentials['OPENAI_API_VERSION']\n",
    "openai.engine = credentials['ENGINE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the data parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "                        'data/new_papers/', \n",
    "                        glob=\"./*.pdf\", \n",
    "                        loader_cls=PyPDFLoader\n",
    "                        )\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='GPT-J 17.8 4.9 31.9\\nGPT-J + CC 19.2 5.6 33.2\\nToolformer (disabled) 22.1 6.3 34.9\\nToolformer 33.8 11.5 53.5\\nOPT (66B) 21.6 2.9 30.1\\nGPT-3 (175B) 26.8 7.0 39.8\\nTable 3: Results on subsets of LAMA. Toolformer uses\\nthe question answering tool for most examples, clearly\\noutperforming all baselines of the same size and achiev-\\ning results competitive with GPT-3 (175B).\\nModel ASDiv SVAMP MAWPS\\nGPT-J 7.5 5.2 9.9\\nGPT-J + CC 9.6 5.0 9.3\\nToolformer (disabled) 14.8 6.3 15.0\\nToolformer 40.4 29.4 44.0\\nOPT (66B) 6.0 4.9 7.9\\nGPT-3 (175B) 14.0 10.0 19.8\\nTable 4: Results for various benchmarks requiring\\nmathematical reasoning. Toolformer makes use of the\\ncalculator tool for most examples, clearly outperform-\\ning even OPT (66B) and GPT-3 (175B).\\nnumber predicted by the model.7\\nTable 4 shows results for all benchmarks. While\\nGPT-J and GPT-J + CC perform about the same,\\nToolformer achieves stronger results even when\\nAPI calls are disabled. We surmise that this is be-', metadata={'source': 'data/new_papers/toolformer.pdf', 'page': 5})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1zuu/anaconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
    "                                                    model_name=\"hkunlp/instructor-xl\", \n",
    "                                                    model_kwargs={\"device\": \"mps\"}\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db/02'\n",
    "\n",
    "embedding = instructor_embeddings\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "                                documents=texts, \n",
    "                                embedding=embedding,\n",
    "                                persist_directory=persist_directory\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(\n",
    "                persist_directory=persist_directory, \n",
    "                embedding_function=embedding\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and', metadata={'page': 1, 'source': 'data/new_papers/Flash-attention.pdf'}),\n",
       " Document(page_content='•Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves\\ntheir quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\\n6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\\nenables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,\\nsolely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\\nto scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\\nperformance on Path-256.\\n•Benchmarking Attention. FlashAttention is up to 3\\x02faster than the standard attention implemen-\\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\\nFlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas', metadata={'page': 2, 'source': 'data/new_papers/Flash-attention.pdf'}),\n",
       " Document(page_content='aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\\nan approximate attention algorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \\x02speedup on\\nGPT-2 (seq. length 1K), and 2.4 \\x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models', metadata={'page': 0, 'source': 'data/new_papers/Flash-attention.pdf'}),\n",
       " Document(page_content='FlashAttention : Fast and Memory-Eﬃcient Exact Attention\\nwith IO-Awareness\\nTri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher Réy\\nyDepartment of Computer Science, Stanford University\\nzDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY\\n{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\\nchrismre@cs.stanford.edu\\nJune 24, 2022\\nAbstract\\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity\\nof self-attention are quadratic in sequence length. Approximate attention methods have attempted\\nto address this problem by trading oﬀ model quality to reduce the compute complexity, but often do\\nnot achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\\naware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes', metadata={'page': 0, 'source': 'data/new_papers/Flash-attention.pdf'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Flash attention?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access.\n",
      "We propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\n",
      "memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\n",
      "This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\n",
      "intermediate attention matrix for the backward pass. We apply two well-established techniques to address\n",
      "these challenges. (i) We restructure the attention computation to split the input into blocks and make several\n",
      "passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\n",
      "store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\n",
      "backward pass, which is faster than the standard approach of reading the intermediate attention matrix from\n",
      "HBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the top 3 documents\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                engine = os.environ[\"ENGINE\"],\n",
    "                model='gpt-3.5-turbo',\n",
    "                temperature=0.9, \n",
    "                max_tokens = 256\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=retriever, \n",
    "                                    return_source_documents=True\n",
    "                                    )\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is a new attention algorithm that improves the efficiency and performance of Transformers, a\n",
      "type of neural network model. It achieves this by reducing the number of memory accesses required during\n",
      "attention computation. FlashAttention avoids reading and writing the attention matrix from high bandwidth\n",
      "memory (HBM) and uses two techniques to optimize its computation. Firstly, it splits the input into blocks and\n",
      "performs incremental softmax reduction, known as tiling. Secondly, it stores the softmax normalization factor\n",
      "from the forward pass to quickly recompute attention on-chip during the backward pass. FlashAttention allows\n",
      "Transformers to process longer sequences, resulting in higher-quality models and improved performance on\n",
      "various tasks. It is faster and more memory efficient than existing attention methods, particularly for\n",
      "sequence lengths up to 512. Additionally, FlashAttention achieves faster training times compared to baseline\n",
      "methods on models such as BERT-large, GPT-2, and long-range arena.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Flash attention?',\n",
       " 'result': 'FlashAttention is a new attention algorithm that improves the efficiency and performance of Transformers, a type of neural network model. It achieves this by reducing the number of memory accesses required during attention computation. FlashAttention avoids reading and writing the attention matrix from high bandwidth memory (HBM) and uses two techniques to optimize its computation. Firstly, it splits the input into blocks and performs incremental softmax reduction, known as tiling. Secondly, it stores the softmax normalization factor from the forward pass to quickly recompute attention on-chip during the backward pass. FlashAttention allows Transformers to process longer sequences, resulting in higher-quality models and improved performance on various tasks. It is faster and more memory efficient than existing attention methods, particularly for sequence lengths up to 512. Additionally, FlashAttention achieves faster training times compared to baseline methods on models such as BERT-large, GPT-2, and long-range arena.',\n",
       " 'source_documents': [Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and', metadata={'page': 1, 'source': 'data/new_papers/Flash-attention.pdf'}),\n",
       "  Document(page_content='•Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves\\ntheir quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\\n6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\\nenables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [ 80] challenge,\\nsolely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\\nto scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\\nperformance on Path-256.\\n•Benchmarking Attention. FlashAttention is up to 3\\x02faster than the standard attention implemen-\\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\\nFlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas', metadata={'page': 2, 'source': 'data/new_papers/Flash-attention.pdf'}),\n",
       "  Document(page_content='aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\\nan approximate attention algorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \\x02speedup on\\nGPT-2 (seq. length 1K), and 2.4 \\x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models', metadata={'page': 0, 'source': 'data/new_papers/Flash-attention.pdf'})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolformer is a language model that has been trained to use external tools via simple APIs. It is designed to\n",
      "decide which APIs to call, when to call them, what arguments to pass, and how to incorporate the results into\n",
      "future token prediction. The goal of Toolformer is to combine the remarkable abilities of language models to\n",
      "solve new tasks with the functionality of external tools. It learns to use tools in a self-supervised way,\n",
      "requiring only a few demonstrations for each API. The use of tools by Toolformer improves the zero-shot\n",
      "performance of language models and enables them to outperform larger models on various downstream tasks.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data/new_papers/toolformer.pdf\n",
      "data/new_papers/toolformer.pdf\n",
      "data/new_papers/toolformer.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"What is toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
