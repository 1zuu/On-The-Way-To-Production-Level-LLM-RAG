{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import together, os, yaml\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Extra, Field, root_validator\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = credentials['HUGGINGFACEHUB_API_TOKEN']\n",
    "os.environ['TOGETHER_AI_API'] = credentials['TOGETHER_AI_API']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austism/chronos-hermes-13b\n",
      "EleutherAI/llemma_7b\n",
      "Gryphe/MythoMax-L2-13b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "NousResearch/Nous-Hermes-Llama2-70b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NumbersStation/nsql-llama-2-7B\n",
      "Open-Orca/Mistral-7B-OpenOrca\n",
      "Phind/Phind-CodeLlama-34B-Python-v1\n",
      "Phind/Phind-CodeLlama-34B-v2\n",
      "SG161222/Realistic_Vision_V3.0_VAE\n",
      "WizardLM/WizardCoder-15B-V1.0\n",
      "WizardLM/WizardLM-70B-V1.0\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-7b-v1.5\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-v0.1\n",
      "prompthero/openjourney\n",
      "runwayml/stable-diffusion-v1-5\n",
      "stabilityai/stable-diffusion-2-1\n",
      "stabilityai/stable-diffusion-xl-base-1.0\n",
      "teknium/OpenHermes-2-Mistral-7B\n",
      "teknium/OpenHermes-2p5-Mistral-7B\n",
      "togethercomputer/CodeLlama-13b-Instruct\n",
      "togethercomputer/CodeLlama-13b-Python\n",
      "togethercomputer/CodeLlama-13b\n",
      "togethercomputer/CodeLlama-34b-Instruct\n",
      "togethercomputer/CodeLlama-34b-Python\n",
      "togethercomputer/CodeLlama-34b\n",
      "togethercomputer/CodeLlama-7b-Instruct\n",
      "togethercomputer/CodeLlama-7b-Python\n",
      "togethercomputer/CodeLlama-7b\n",
      "togethercomputer/GPT-JT-6B-v1\n",
      "togethercomputer/GPT-JT-Moderation-6B\n",
      "togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/Llama-2-7B-32K-Instruct\n",
      "togethercomputer/Pythia-Chat-Base-7B-v0.16\n",
      "togethercomputer/Qwen-7B-Chat\n",
      "togethercomputer/Qwen-7B\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/alpaca-7b\n",
      "togethercomputer/falcon-40b-instruct\n",
      "togethercomputer/falcon-40b\n",
      "togethercomputer/falcon-7b-instruct\n",
      "togethercomputer/falcon-7b\n",
      "togethercomputer/llama-2-13b-chat\n",
      "togethercomputer/llama-2-13b\n",
      "togethercomputer/llama-2-70b-chat\n",
      "togethercomputer/llama-2-70b\n",
      "togethercomputer/llama-2-7b-chat\n",
      "togethercomputer/llama-2-7b\n",
      "upstage/SOLAR-0-70b-16bit\n",
      "wavymulder/Analog-Diffusion\n",
      "EleutherAI/pythia-1b-v0\n",
      "lmsys/vicuna-13b-v1.3\n",
      "togethercomputer/codegen2-16B\n",
      "togethercomputer/replit-code-v1-3b\n",
      "togethercomputer/mpt-7b\n",
      "togethercomputer/mpt-30b-chat\n",
      "google/flan-t5-xxl\n",
      "google/flan-t5-xl\n",
      "togethercomputer/mpt-7b-instruct\n",
      "NumbersStation/nsql-6B\n",
      "togethercomputer/Koala-7B\n",
      "EleutherAI/pythia-6.9b\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "EleutherAI/gpt-neox-20b\n",
      "EleutherAI/pythia-2.8b-v0\n",
      "NousResearch/Nous-Hermes-13b\n",
      "togethercomputer/guanaco-65b\n",
      "OpenAssistant/oasst-sft-6-llama-30b-xor\n",
      "Salesforce/instructcodet5p-16b\n",
      "lmsys/fastchat-t5-3b-v1.0\n",
      "huggyllama/llama-7b\n",
      "OpenAssistant/stablelm-7b-sft-v7-epoch-3\n",
      "EleutherAI/pythia-12b-v0\n",
      "togethercomputer/mpt-7b-chat\n",
      "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "EleutherAI/gpt-j-6b\n",
      "lmsys/vicuna-7b-v1.3\n",
      "togethercomputer/codegen2-7B\n",
      "huggyllama/llama-13b\n",
      "togethercomputer/guanaco-13b\n",
      "HuggingFaceH4/starchat-alpha\n",
      "huggyllama/llama-30b\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "defog/sqlcoder\n",
      "bigcode/starcoder\n",
      "databricks/dolly-v2-7b\n",
      "togethercomputer/guanaco-33b\n",
      "togethercomputer/Koala-13B\n",
      "togethercomputer/guanaco-7b\n"
     ]
    }
   ],
   "source": [
    "together.api_key = os.environ[\"TOGETHER_AI_API\"]\n",
    "\n",
    "# list available models and descriptons\n",
    "models = together.Models.list()\n",
    "for m in models:\n",
    "    print(m['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'value': '9f80dbe75ee2d9408b637393a9a3081395fa2dcd007b7ae130c61ebf88aee09e-1802c08ee7b2bdd56c3c7c9853a8ea9272433524cc037566a9a16aff837e285e'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together.Models.start(\"togethercomputer/llama-2-13b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-7b-chat\" # model endpoint to use\n",
    "    together_ai_api: str = os.environ[\"TOGETHER_AI_API\"] # Together API key\n",
    "    temperature: float = 0.7 # What sampling temperature to use.\n",
    "    max_tokens: int = 512 # The maximum number of tokens to generate in the completion.\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set.\"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_ai_api\", \"TOGETHER_AI_API\"\n",
    "        )\n",
    "        values[\"together_ai_api\"] = api_key\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        together.api_key = self.together_ai_api\n",
    "        output = together.Complete.create(prompt,\n",
    "                                          model=self.model,\n",
    "                                          max_tokens=self.max_tokens,\n",
    "                                          temperature=self.temperature,\n",
    "                                          )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "                        'data/new_papers/', \n",
    "                        glob=\"./*.pdf\", \n",
    "                        loader_cls=PyPDFLoader\n",
    "                        )\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "model_norm = HuggingFaceBgeEmbeddings(\n",
    "                                    model_name=model_name,\n",
    "                                    model_kwargs={'device': 'mps'},\n",
    "                                    encode_kwargs=encode_kwargs\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db/05'\n",
    "\n",
    "## Here is the nmew embeddings being used\n",
    "embedding = model_norm\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "                                documents=texts,\n",
    "                                embedding=embedding,\n",
    "                                persist_directory=persist_directory\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default LLaMA-2 prompt style\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
      "<</SYS>>\n",
      "\n",
      "CONTEXT:/n/n {context}/n\n",
      "\n",
      "Question: {question}[/INST]\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "print(get_prompt(instruction, sys_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM(\n",
    "                model= \"togethercomputer/llama-2-7b-chat\",\n",
    "                temperature = 0.1,\n",
    "                max_tokens = 1024\n",
    "                )\n",
    "\n",
    "prompt_template = get_prompt(instruction, sys_prompt)\n",
    "\n",
    "llama_prompt = PromptTemplate(\n",
    "                            template=prompt_template, \n",
    "                            input_variables=[\"context\", \"question\"]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=retriever,\n",
    "                                    return_source_documents=True,\n",
    "                                    chain_type_kwargs={\"prompt\": llama_prompt}\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FlashAttention is a type of exact attention algorithm that is designed to be fast and memory-efficient. It is\n",
      "based on the idea of tiling, which reduces the number of memory reads and writes required for attention\n",
      "computations. FlashAttention is faster than other exact attention methods, including approximate attention\n",
      "methods, and has a smaller memory footprint. It is also more memory-efficient than other exact attention\n",
      "methods, including approximate attention methods, and has a smaller memory footprint. FlashAttention is based\n",
      "on the principle of IO-awareness, which means that it is designed to account for reads and writes between\n",
      "levels of GPU memory. This allows it to be more efficient in terms of both time and memory usage.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context window for LLaMA-2 is 512.\n",
      "\n",
      "Unhelpful Answer: The context window for LLaMA-2 is 1024.\n",
      "\n",
      "Note: The context window is the number of tokens in the input sequence that the model can use to make\n",
      "predictions.\n",
      "\n",
      "\n",
      "Sources:\n",
      "data/new_papers/ALiBi.pdf\n",
      "data/new_papers/ALiBi.pdf\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/Flash-attention.pdf\n",
      "data/new_papers/ALiBi.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the context window for LLaMA-2?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
